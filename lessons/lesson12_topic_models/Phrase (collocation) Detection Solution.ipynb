{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase (collocation) Detection Solution\n",
    "\n",
    "###### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "1. Acronym replacement\n",
    "2. SpaCy POS phrases\n",
    "3. Gensim Phrases and Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import defaultdict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from IPython.core.display import display, HTML\n",
    "from configparser import ConfigParser, ExtendedInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for data, acronyms, and gensim paths\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']\n",
    "AIRLINE_ACRONYMS_FILEPATH = config['NLP']['AIRLINE_ACRONYMS_FILEPATH']\n",
    "AIRLINE_MATCHED_TEXT_PATH = config['NLP']['AIRLINE_MATCHED_TEXT_PATH']\n",
    "AIRLINE_CLEANED_TEXT_PATH = config['NLP']['AIRLINE_CLEANED_TEXT_PATH']\n",
    "GENSIM_DICTIONARY_PATH = config['NLP']['GENSIM_DICTIONARY_PATH']\n",
    "GENSIM_CORPUS_PATH = config['NLP']['GENSIM_CORPUS_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data on airline fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>section_name</th>\n",
       "      <th>section_text</th>\n",
       "      <th>criteria</th>\n",
       "      <th>section_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>292</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>DEPARTMENT OF TRANSPORTATION RANKINGS FOR 1994...</td>\n",
       "      <td>A multitude of challenges faced the People of ...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>2849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>298</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>RESULTS OF OPERATIONS</td>\n",
       "      <td>1994 COMPARED WITH 1993 The Company's consolid...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>13806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>306</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>ACQUISITION</td>\n",
       "      <td>On December 31, 1993, Southwest exchanged 3,57...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>309</td>\n",
       "      <td>southwest-airlines-co_annual_report_1994.docx</td>\n",
       "      <td>ACCRUED LIABILITIES (IN THOUSANDS) LONG-TERM D...</td>\n",
       "      <td>On March 1, 1993, the Company redeemed the $10...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>360</td>\n",
       "      <td>southwest-airlines-co_annual_report_1995.docx</td>\n",
       "      <td>SECRET NUMBER 1 STICK TO WHAT YOU’RE GOOD AT.</td>\n",
       "      <td>Since 1971, Southwest Airlines has offered sin...</td>\n",
       "      <td>&lt;, f, u, n, c, t, i, o, n,  , s, t, y, l, e,  ...</td>\n",
       "      <td>2566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     section_id                                       filename  \\\n",
       "291         292  southwest-airlines-co_annual_report_1994.docx   \n",
       "297         298  southwest-airlines-co_annual_report_1994.docx   \n",
       "305         306  southwest-airlines-co_annual_report_1994.docx   \n",
       "308         309  southwest-airlines-co_annual_report_1994.docx   \n",
       "359         360  southwest-airlines-co_annual_report_1995.docx   \n",
       "\n",
       "                                          section_name  \\\n",
       "291  DEPARTMENT OF TRANSPORTATION RANKINGS FOR 1994...   \n",
       "297                              RESULTS OF OPERATIONS   \n",
       "305                                        ACQUISITION   \n",
       "308  ACCRUED LIABILITIES (IN THOUSANDS) LONG-TERM D...   \n",
       "359      SECRET NUMBER 1 STICK TO WHAT YOU’RE GOOD AT.   \n",
       "\n",
       "                                          section_text  \\\n",
       "291  A multitude of challenges faced the People of ...   \n",
       "297  1994 COMPARED WITH 1993 The Company's consolid...   \n",
       "305  On December 31, 1993, Southwest exchanged 3,57...   \n",
       "308  On March 1, 1993, the Company redeemed the $10...   \n",
       "359  Since 1971, Southwest Airlines has offered sin...   \n",
       "\n",
       "                                              criteria  section_length  \n",
       "291  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...            2849  \n",
       "297  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...           13806  \n",
       "305  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...            2141  \n",
       "308  <, f, u, n, c, t, i, o, n,  , h, e, a, d, i, n...            1855  \n",
       "359  <, f, u, n, c, t, i, o, n,  , s, t, y, l, e,  ...            2566  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = create_engine(DB_PATH)\n",
    "df = pd.read_sql(\"SELECT * FROM Sections\", con=engine)\n",
    "\n",
    "# the annual report from 1992 was scanned in poor quality\n",
    "# and the text was not legible\n",
    "df = df[df.filename != 'southwest-airlines-co_annual_report_1992.docx']\n",
    "\n",
    "# filter to relevant sections\n",
    "df = df[df['section_text'].str.contains('fee')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A multitude of challenges faced the People of Southwest Airlines in 1994. The mark of a true champion is the ability to “rise to the occasion” and meet challenges. We believe our Employees showed their true Southwest Spirit in 1994, accomplishing three- or four-fold what a normal year would  bring.'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store section matches in list\n",
    "text = [section for section in df['section_text'].values]\n",
    "\n",
    "# review first sentence of a section match\n",
    "text[0][0:299]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load spacy nlp model\n",
    "# use 'en' if you don't have the lg model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Preprocessing - Acronyms\n",
    "\n",
    "SOURCE: https://www.faa.gov/airports/resources/acronyms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acronym</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/C</td>\n",
       "      <td>Aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/G</td>\n",
       "      <td>Air to Ground</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/H</td>\n",
       "      <td>Altitude/Height</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Acronym        Definition\n",
       "0     A/C          Aircraft\n",
       "1     A/G     Air to Ground\n",
       "2     A/H   Altitude/Height"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv with airline industry acronyms\n",
    "airline_acronyms = pd.read_csv(AIRLINE_ACRONYMS_FILEPATH)\n",
    "airline_acronyms.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "**Curate the acronyms:**\n",
    "\n",
    "1. Convert the acronyms into a dict\n",
    "2. Clean acronyms and definitions (replace spaces with underscores, strip text, lowercase)\n",
    "3. Remove any acronyms that are < two characters (e.g. 'at' == 'air traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('agl', 'above_ground_level'),\n",
       " ('afb', 'air_force_base'),\n",
       " ('aig', 'airbus_industries_group'),\n",
       " ('iap', 'instrument_approach_procedures'),\n",
       " ('malsr', 'mals_with_runway_alignment_indicator_lights')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acronyms = {}\n",
    "\n",
    "for ind, row in airline_acronyms.iterrows():\n",
    "    # convert acronyms to lowercase\n",
    "    acronym = row['Acronym'].lower()\n",
    "    \n",
    "    # clean acronym definitions\n",
    "    definition = row['Definition'].lower().strip().replace(' ','_')\n",
    "    \n",
    "    # ignore two character acronyms as they often match actual words\n",
    "    # e.g. 'at' == 'air traffic'\n",
    "    if len(acronym) > 2:\n",
    "        acronyms[acronym] = definition\n",
    "\n",
    "# view the first few acronyms\n",
    "list(acronyms.items())[0:5]  # convert to list as dict is not subscriptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify acronyms that exist in text\n",
    "WARNING SLOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 'clear')\n",
      "('rnp', 'required_navigation_performance')\n",
      "('mou', 'memorandum_of_understanding')\n",
      "('app', 'approach')\n",
      "('grade', 'graphical_airspace_design_environment')\n",
      "('asm', 'available_seat_mile')\n",
      "('tops', 'telecommunications_ordering_and_pricing_system_(gsa_software_tool)')\n",
      "('dot', 'department_of_transportation')\n",
      "('tsa', 'taxiway_safety_area')\n",
      "('par', 'preferential_arrival_route')\n",
      "('ata', 'air_transport_association_of_america')\n",
      "('aid', 'airport_information_desk')\n",
      "('faa', 'federal_aviation_administration')\n",
      "('self', 'simplified_short_approach_lighting_system_with_sequenced_flashing_lights')\n",
      "('basic', 'basic_contract_observing_station')\n",
      "('gps', 'global_positioning_system')\n",
      "('did', 'direct_inward_dial')\n",
      "('far', 'federal_aviation_regulation')\n",
      "Wall time: 11min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# review the acronyms\n",
    "acronym_matches = []\n",
    "\n",
    "# create a nlp pipe to iterate through the text\n",
    "for doc in nlp.pipe(text, disable=['tagger','ner']):    \n",
    "    # iterate through the sentences of the doc\n",
    "    for sent in doc.sents:\n",
    "        # iterate through each word in the sentence\n",
    "        for token in sent.text.split(' '):\n",
    "            token = token.lower()\n",
    "            # check if token is an acronym\n",
    "            # add matches (acronym and definition) to acronym_matches\n",
    "            if token in acronyms:\n",
    "                acronym_matches.append((token, acronyms[token]))\n",
    "\n",
    "# review all matching acronyms      \n",
    "for match in set(acronym_matches):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update acronyms list to remove ambiguous acronyms\n",
    "acronyms_to_remove = ['cat','app','grade','self','basic','did','far']\n",
    "for term in acronyms_to_remove:\n",
    "    acronyms.pop(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### collect sentences about fees for phrase model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_phrase_model_sents(matcher, doc, i, matches):\n",
    "    # identify matching spans (phrases)\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start : end]\n",
    "    \n",
    "    # keep only words, lemmatize tokens, remove punctuation\n",
    "    sent = [str(token.lemma_).lower() \n",
    "            for token in span.sent if token.is_alpha]\n",
    "    \n",
    "    # replace acronyms\n",
    "    sent = [acronyms[token] if token in acronyms \n",
    "            else token for token in sent]\n",
    "\n",
    "    # collect matching (cleaned) sents\n",
    "    matched_sents.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### match sentences with the word fee or fees\n",
    "\n",
    "WARNING SLOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# match sentences with the word fee or fees\n",
    "matched_sents = []\n",
    "pattern = [[{'LOWER': 'fee'}], [{'LOWER': 'fees'}]]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# use *patterns to add more than one pattern at once\n",
    "matcher.add('fees', collect_phrase_model_sents, *pattern)\n",
    "\n",
    "for doc in nlp.pipe(text, disable=['tagger','ner']):    \n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 454 \n",
      "\n",
      "Example Match:\n",
      "['rather', 'than', 'pay', 'the', 'fee', 'demand', 'by', 'this', 'crss', 'we', 'respond', 'quickly', 'with', 'our', 'own', 'travel', 'agency', 'solution', 'direct', 'access', 'and', 'ticket', 'for', 'the', 'large', 'agency', 'swat', 'overnight', 'delivery', 'of', 'southwest', 'produce', 'ticket', 'for', 'approximately', 'large', 'travel', 'agency', 'improve', 'access', 'to', 'ticket', 'by', 'mail', 'for', 'direct', 'customers', 'by', 'reduce', 'the', 'time', 'limit', 'from', 'seven', 'day', 'out', 'from', 'the', 'date', 'of', 'travel', 'to', 'three', 'day', 'and', 'ticketless', 'travel', 'which', 'eliminate', 'the', 'need', 'to', 'print', 'a', 'paper', 'ticket', 'altogether']\n"
     ]
    }
   ],
   "source": [
    "print('Number of matches: {} \\n'.format(len(matched_sents)))\n",
    "\n",
    "print('Example Match:')\n",
    "print(matched_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export matched text to avoid repeating processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the matched text to a .txt file for later use\n",
    "with open(AIRLINE_MATCHED_TEXT_PATH, 'w') as f:\n",
    "    for line in matched_sents:\n",
    "        line = ' '.join(line) + '\\n'\n",
    "        line = line.encode('ascii', errors='ignore').decode('ascii')\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read matched text\n",
    "with open(AIRLINE_MATCHED_TEXT_PATH, 'r') as f:\n",
    "    matched_sents_full = [line for line in f.readlines()]\n",
    "    matched_sents = [line.split() for line in matched_sents_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rather than pay the fee demand by this crss we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these expense include million of various profe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>included in this one time cost result from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the commitment fee be per annum\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>landing fee and other rental per available_sea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences\n",
       "0  rather than pay the fee demand by this crss we...\n",
       "1  these expense include million of various profe...\n",
       "2  included in this one time cost result from the...\n",
       "3                  the commitment fee be per annum\\n\n",
       "5  landing fee and other rental per available_sea..."
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store all matched senteces in a dataframe\n",
    "matches_df = pd.DataFrame(matched_sents_full, columns=['sentences'])\n",
    "\n",
    "# remove duplicates\n",
    "matches_df = matches_df[~matches_df.duplicated()]\n",
    "\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SpaCy part of speech (POS) to create phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rather than pay the fee demand by this crss we respond quickly with our own travel agency solution direct access and ticket for the large agency swat overnight delivery of southwest produce ticket for approximately large travel agency improve access to ticket by mail for direct customers by reduce the time limit from seven day out from the date of travel to three day and ticketless travel which eliminate the need to print a paper ticket altogether'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the matched sentence tokens and parse it with SpaCy\n",
    "text = ' '.join(matched_sents[0])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine which NLP components can be disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pos(doc, n_tokens=5):\n",
    "    \"\"\" print SpaCy POS information about each token in a provided document \"\"\"\n",
    "    print('{:15} | {:10} | {:10} | {:30}'.format('TOKEN','POS','DEP_','LEFTS'))\n",
    "    for token in doc[0:n_tokens]:\n",
    "        print('{:15} | {:10} | {:10} | {:30}'.format(\n",
    "            token.text, token.head.pos_,token.dep_, str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "rather          | ADP        | advmod     | []                            \n",
      "than            | VERB       | advmod     | ['rather']                    \n",
      "pay             | VERB       | ROOT       | ['than']                      \n",
      "the             | NOUN       | det        | []                            \n",
      "fee             | NOUN       | compound   | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by named entity recognition (ner)\n",
    "pos_doc = nlp(text, disable=['ner'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "rather          | ADV        |            | []                            \n",
      "than            | ADP        |            | []                            \n",
      "pay             | VERB       |            | []                            \n",
      "the             | DET        |            | []                            \n",
      "fee             | NOUN       |            | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by parser\n",
    "pos_doc = nlp(text, disable=['ner','parser'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           | POS        | DEP_       | LEFTS                         \n",
      "rather          |            | advmod     | []                            \n",
      "than            |            | advmod     | ['rather']                    \n",
      "pay             |            | ROOT       | ['than']                      \n",
      "the             |            | det        | []                            \n",
      "fee             |            | compound   | []                            \n",
      "demand          |            | dobj       | ['the', 'fee']                \n",
      "by              |            | prep       | []                            \n",
      "this            |            | det        | []                            \n",
      "crss            |            | pobj       | ['this']                      \n",
      "we              |            | nsubj      | []                            \n"
     ]
    }
   ],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by tagger\n",
    "pos_doc = nlp(text, disable=['ner','tagger'])\n",
    "view_pos(pos_doc, n_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use explain to define any token.dep_ attributes\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/api/annotation#dependency-parsing width=1000 height=400></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_parsing_labels_url = 'https://spacy.io/api/annotation#dependency-parsing'\n",
    "iframe = '<iframe src={} width=1000 height=400></iframe>'.format(dependency_parsing_labels_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract phrases by identifying tokens describing an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agency_swat the_date large_swat the_demand the_need agency_solution produce_ticket this_crss travel_agency time_limit direct_customer seven_day our_solution the_limit paper_ticket three_day large_agency the_swat own_solution a_ticket fee_demand\n"
     ]
    }
   ],
   "source": [
    "def create_pos_phrases(doc):\n",
    "\n",
    "    doc = nlp(doc, disable=['ner','tagger'])\n",
    "    \n",
    "    phrases = [] \n",
    "    for token in doc:\n",
    "        # find any objects (e.g. direct objects )\n",
    "        if 'obj' in token.dep_:\n",
    "            token_text = token.lemma_.lower()\n",
    "            # find any dependent terms to the left of (preceeding) the object\n",
    "            for left_term in [t.text for t in token.lefts]:\n",
    "                phrase = '{}_{}'.format(left_term,token_text)\n",
    "                phrases.append(phrase)\n",
    "    \n",
    "    return ' '.join(set(phrases))\n",
    "\n",
    "print(create_pos_phrases(matched_sents_full[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# apply the custom function to every element in the dataframe\n",
    "matches_df['pos_phrases'] = matches_df.sentences.apply(create_pos_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>pos_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rather than pay the fee demand by this crss we...</td>\n",
       "      <td>agency_swat the_date large_swat the_demand the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these expense include million of various profe...</td>\n",
       "      <td>relocation_cost professional_million employee_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>included in this one time cost result from the...</td>\n",
       "      <td>relocation_cost employee_cost this_result dupl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the commitment fee be per annum\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>landing fee and other rental per available_sea...</td>\n",
       "      <td>airport_credit a_credit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  rather than pay the fee demand by this crss we...   \n",
       "1  these expense include million of various profe...   \n",
       "2  included in this one time cost result from the...   \n",
       "3                  the commitment fee be per annum\\n   \n",
       "5  landing fee and other rental per available_sea...   \n",
       "\n",
       "                                         pos_phrases  \n",
       "0  agency_swat the_date large_swat the_demand the...  \n",
       "1  relocation_cost professional_million employee_...  \n",
       "2  relocation_cost employee_cost this_result dupl...  \n",
       "3                                                     \n",
       "5                            airport_credit a_credit  "
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas Apply\n",
    "\n",
    "apply is an efficient and fast approach to 'apply' a function to every element in a row. applymap does the same to every element in the entire dataframe (e.g. convert all ints to floats)\n",
    "\n",
    "Example: https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     0     3\n",
       "1     1     4\n",
       "2     2     5"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a small dataframe with example data\n",
    "test_df = pd.DataFrame({'col1':range(0,3),'col2':range(3,6)})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "Name: col1, dtype: float64"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a built-in function to each element in a column\n",
    "test_df['col1'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "Name: col1, dtype: int64"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a custom function to every element in a column\n",
    "def add_five(row):\n",
    "    return row + 5\n",
    "\n",
    "test_df['col1'].apply(add_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "Name: col1, dtype: int64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply an annonomous function to every element in a column\n",
    "test_df['col1'].apply(lambda x: x+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0   0.0   3.0\n",
       "1   1.0   4.0\n",
       "2   2.0   5.0"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a built-in function to every element in a dataframe \n",
    "test_df.applymap(float)  # applymap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase (collocation) Detection\n",
    "\n",
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus in order\n",
    "- $N$ is the total size of the corpus vocabulary\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular.\n",
    "\n",
    "SOURCE: https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-learn API for Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhrasesTransformer(delimiter=b'_', max_vocab_size=40000000, min_count=3,\n",
       "          progress_per=10000, scoring='default', threshold=3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.sklearn_api.phrases import PhrasesTransformer\n",
    "\n",
    "sklearn_phrases = PhrasesTransformer(min_count=3, threshold=3)\n",
    "sklearn_phrases.fit(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alsherman\\AppData\\Local\\Continuum\\anaconda3\\envs\\guild\\lib\\site-packages\\gensim\\models\\phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'federal_aviation_administration', 'available_seat_mile', 'per_available_seat_mile', 'available_seat_mile_increase', 'the_department_of_transportation', 'the_taxiway_safety_area', 'required_navigation_performance'}\n"
     ]
    }
   ],
   "source": [
    "# review phrase matches\n",
    "phrases = []\n",
    "for terms in sklearn_phrases.transform(matched_sents):\n",
    "    for term in terms:\n",
    "        if term.count('_') >= 2:\n",
    "            phrases.append(term)\n",
    "print(set(phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim API\n",
    "A more complex API, though it is faster and has better integration with other gensim components (e.g. Phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "common_terms = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**common_terms:** optionnal list of “stop words” that won’t affect frequency count of expressions containing them.\n",
    "    - The common_terms parameter add a way to give special treatment to common terms (aka stop words) such that their presence between two words won’t prevent bigram detection. It allows to detect expressions like “bank of america” or “eye of the beholder”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phrases at 0x1590a88b4a8>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = Phrases(\n",
    "      matched_sents\n",
    "    , common_terms=common_terms\n",
    "    , min_count=3\n",
    "    , threshold=3\n",
    "    , scoring='default'\n",
    ")\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases Params\n",
    "\n",
    "- **scoring:** specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n",
    "\n",
    "    - ‘default’: from “Efficient Estimaton of Word Representations in Vector Space” by Mikolov, et. al.: \n",
    "    \n",
    "$$\\frac{count(AB) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "    \n",
    "\n",
    "    - where N is the total vocabulary size.\n",
    "    - Thus, it is easier to exceed the threshold when the two words occur together often or when the two words are rare (i.e. small product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phraser at 0x1590a88b198>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrases object still contains all the source text in memory. A gensim Phraser will remove this extra data to become smaller and somewhat faster than using the full Phrases model. To determine what data to remove, the Phraser ues the  results of the source model’s min_count, threshold, and scoring settings. (You can tamper with those & create a new Phraser to try other values.)\n",
    "\n",
    "SOURCE: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_phrases(phraser, text_stream, num_underscores=2):\n",
    "    \"\"\" identify phrases from a text stream by searching for terms that\n",
    "    are separated by underscores and include at least num_underscores\n",
    "    \"\"\"\n",
    "    \n",
    "    phrases = []\n",
    "    for terms in phraser[text_stream]:\n",
    "        for term in terms:\n",
    "            if term.count('_') >= num_underscores:\n",
    "                phrases.append(term)\n",
    "    print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item_such_a_seat', 'available_seat_mile', 'conspire_with_delta', 'service_be_provide', 'fare_be_refundable', 'delay_of_much_than_minute', 'follow_the_acquisition', 'subject_to_a_maximum', 'cancellation_or_diversion', 'required_navigation_performance', 'reservation_be_make_at_little', 'fee_must_apply', 'item_such_a_\\ufeff1', 'disclose_all_potential', 'continue_to_be_the_only_major', 'importance_to_southwest', 'cancel_a_pay', 'cost_be_then_allocate', 'serve_to_which_various_leasehold', 'damage_for_the_amount_of_\\ufeff1', 'initiative_a_previously', 'decision_that_create', 'primarily_due_to_high', 'delta_the_consolidated', 'promote_the_company', 'imposition_of_a_\\ufeff1', 'difference_in_airfare', 'rate_and_charge', 'result_in_a_low', 'website_and_iv', 'impact_the_company', 'hold_a_reservation', 'relief_against_a_broad', 'cost_for_safety', 'congress_may_consider', 'customers_on_behalf', 'report_much_information', 'change_in_law', 'fee_for_permanently', 'extension_or_transfer', 'require_that_i_advertise', 'airfare_the_customer', 'legislation_which_can_result', 'grant_to_the_taxiway_safety_area', 'jurisdiction_over_its_operation', '\\ufeff1_or_2', 'service_such_a_the_transportation', 'propose_to_congress', 'pay_a_difference', 'airtran_to_have_a_consistent', 'include_but_not_limit', 'long_a_the_reservation', 'numb_of_total', 'advantage_of_such_service', 'hour_without_make_a_payment', 'percent_of_operating', 'passenger_at_the_time', 'enter_into_fuel', 'limitation_on_route', 'increase_after_purchase', 'fare_may_not_increase', 'lease_and_for_terminal', 'operation_of_customs', 'available_seat_mile_basis', 'duplicate_or_incompatible', 'conjurer_the_only_major', 'law_that_affect', 'hedge_against_increase', 'intend_upon_full_integration', 'day_in_advance', 'operation_can_be_negative', 'respectfully_with_its_low', 'rate_and_the_elimination', 'service_if_a_flight', 'addition_to_treble', 'allocate_among_a_few_numb', 'security_be_provide', 'require_among_other_thing', 'reservation_for_up_to_hour', 'passenger_be_unable', 'pay_a_usage', 'department_of_transportation_far', 'rule_by_implement', 'fee_for_optional', 'project_the_majority', 'limit_or_regulate', 'typically_the_flight', 'reservation_without_penalty', 'cancel_or_oversell', 'affect_through_future', 'share_with_ticket', 'changeable_and_include', 'payment_of_facilities', 'taxiway_safety_area_to_assess', 'rental_per_available_seat_mile', 'airline_that_doe', 'department_of_transportation_on_the_amount_and_type', 'confirmation_and_viii', 'doe_not_charge', 'notify_in_the_event', 'cost_the_taxiway_safety_area', 'necessary_to_cover', 'stand_the_claim', 'pay_to_airtran', 'consist_of_distribution', 'reduce_their_capacity', 'taxiway_safety_area', 'facility_at_each_of_the_airport', 'change_to_environmental', 'passenger_be_allow', 'hour_after_the_reservation', 'use_to_support', 'doe_not_impose', 'remit_this_back_to_the_applicable', 'generate_much_government', 'capacity_and_price', 'penalty_for_hour', 'capacity_or_use', 'unlike_much_of_its_competitor', 'pursuant_to_authority', 'allow_to_cancel', 'person_or_entity', 'department_of_transportation', 'advantage_by_differentiate', 'cost_such_a_credit', 'reservation_be_make_a_long', 'component_of_the_passenger', 'airtran_and_delta', 'maximum_of_per_one_way', 'federal_aviation_administration', 'august_the_court', 'payments_be_expect', 'action_and_decision', 'business_through_the_imposition', 'access_at_slot', 'effort_to_reduce', 'domestic_and_foreign', 'fee_must_be_disclose', 'pay_for_ancillary', 'new_and_expand', 'bump_from_flight', 'conspiracy_with_respect', 'department_of_transportation_propose', 'deduction_and_preference', 'primarily_a_a_result', 'fund_for_passenger', 'information_for_basic', 'grants_to_airport', 'gain_a_competitive', 'plan_can_change', 'permit_and_approval', 'fare_and_the_adoption', 'complete_and_ongoing', 'additionally_when_other_airline', 'purchase_of_miles', 'damage_on_behalf', 'restriction_on_competitive', 'attempt_to_monopolize', 'range_of_allege', 'advance_of_travel', 'fare_and_no_unexpected', 'refund_any_check', 'curb_side_checkin', 'measure_have_conjurer', 'impose_a_annual', 'service_on_their_website', 'million_or_percent', 'passenger_must_be_promptly', 'minimum_of_per_one_way', 'generate_much_net', 'asif_on_each_airline', 'thing_that_airtran', 'confirmation_and_vii', '\\ufeff1_and_2', 'finance_may_conjurer', 'difficulty_in_obtain', 'change_and_therefore_doe', 'fee_and_other_rental', 'allow_to_hold', 'primarily_due_to_consult', 'violation_of_section', 'include_in_aircraft', 'pay_up_to_in_deny', 'charge_a_change', 'reimbursement_of_the_company', 'taxis_and_fee', 'offer_by_airline'}\n"
     ]
    }
   ],
   "source": [
    "print_phrases(bigram, matched_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram phrase model\n",
    "\n",
    "We can place the text from the first phrase model into another Phrases object to create n-term phrase models. We can repear this process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attempt_to_monopolize_air_travel', 'item_such_a_seat', 'available_seat_mile', 'effective_july_and_ii_eliminate', 'capacity_and_price_decision', 'include_in_aircraft_rental', 'conjurer_the_only_major_airline_that_doe', 'flight_ii_refund_any_check', 'airtran_currently_charge', 'fare_be_refundable', 'customer_change_in_flight', 'follow_the_acquisition', 'required_navigation_performance', 'amounts_collect_from_passenger', 'passenger_be_allow_to_cancel_a_pay', 'generate_much_net_federal_revenue', 'continue_to_be_the_only_major', 'southwest_be_conjurer_the_only_major', 'anticompetitive_activity_a_good', 'confirmation_and_viii_passenger_must_be_promptly', 'purchase_of_miles_rewards', 'initial_complaint_seek_treble', 'serve_to_which_various_leasehold', 'initiative_a_previously', 'decision_that_create', 'primarily_due_to_high', 'pay_for_ancillary_service_if_a_flight', 'promote_the_company', 'relief_against_a_broad_range_of_allege', 'addition_to_treble_damage_for_the_amount_of_\\ufeff1', 'difference_in_airfare', 'collect_certain_taxis_and_fee', 'rate_and_charge', 'result_in_a_low', 'website_and_iv', 'impact_the_company', 'curb_side_checkin_and_telephone_reservation', 'allow_to_cancel_a_pay_reservation', 'reservation_for_up_to_hour_without_make_a_payment_iii', 'landing_fee_and_other_rental_respectively', 'cost_for_safety', 'congress_may_consider', 'customers_on_behalf', 'report_much_information', 'ii_passenger_be_allow_to_hold', 'bag_fee_for_permanently', 'campaign_highlight_the_importance_to_southwest', 'approve_legislation_in_december', 'lease_and_for_terminal_operation_lease', 'change_in_law', 'directly_pay_delta_airtran', 'aviation_security_infrastructure_fee', 'pursuant_to_authority_grant_to_the_taxiway_safety_area', 'generate_much_government_revenue_congress', '\\ufeff1_item_of_check_luggage', 'priority_seat_selection_special', 'treat_customers_fairly_honestly', 'offer_by_airline', 'legislation_which_can_result', 'august_the_court_dismiss_claim', 'jurisdiction_over_its_operation', 'generally_recognize_a_other_revenue', '\\ufeff1_or_2', 'atsa_fund_for_passenger', 'air_carrier_if_necessary_to_cover', 'landing_fee_and_other_rental_per_available_seat_mile', 'consolidated_statement_of_income', 'propose_to_congress', 'airtran_to_have_a_consistent', 'include_but_not_limit', 'stand_the_claim_of_a_conspiracy_with_respect', 'refund_any_check_bag', 'respectfully_with_its_low_fare_and_no_unexpected', 'advantage_of_such_service', 'percent_of_operating', 'security_be_provide_in_part_by_a_per_enplanement_security', 'cost_be_then_allocate_among_a_few_numb_of_total', 'aircraft_operate_lease_and_for_terminal', 'item_such_a_\\ufeff1_and_2_check', 'operation_lease_expense', 'cancel_or_oversell_and_a_passenger_be_unable', 'pay_by_airline_directly', 'enter_into_fuel', 'limitation_on_route', 'company_currently_expect_landing', 'penalty_for_hour_after_the_reservation_be_make_a_long', 'fee_must_be_disclose_on_e_ticket', 'operation_of_customs', 'rental_and_in_landing_fee', 'available_seat_mile_basis', 'duplicate_or_incompatible', 'law_that_affect', 'hedge_against_increase', 'intend_upon_full_integration', 'fare_may_not_increase_after_purchase_v', 'passenger_at_the_time_of_book_vi', 'operation_can_be_negative', 'rate_and_the_elimination', 'rental_per_available_seat_mile_increase_percent', 'day_in_advance_of_travel_iv', 'allocate_among_a_few_numb', 'lose_luggage_iii_prominently', 'landing_fee_and_other_rental_expense', 'reservation_without_penalty_for_hour_after_the_reservation', 'violation_of_section_of_the_sherman_act', 'passenger_trip_vii_baggage', 'pay_a_usage', 'service_on_their_website_and_iv_refund', 'way_passenger_trip', 'selection_fuel_surcharge_snack', 'department_of_transportation_far', 'rule_by_implement', 'confirmation_and_vii_passenger_must_be_promptly', 'project_the_majority', 'customers_on_behalf_of_government_agency', 'limit_or_regulate', 'customer_may_pay_a_difference', 'cost_associate_with_complete_and_ongoing', 'bump_from_flight_ii_refund', 'affect_through_future', 'share_with_ticket', 'changeable_and_include', 'conspiracy_with_respect_to_the_imposition_of_a_\\ufeff1', 'long_a_the_reservation_be_make_at_little_seven', 'payment_of_facilities', 'taxiway_safety_area_to_assess', 'rental_per_available_seat_mile', 'airline_that_doe', 'rental_respectively_in_the_consolidated_statement', 'flight_change_seat', 'department_of_transportation_on_the_amount_and_type', 'violate_section_of_the_sherman_act', 'plan_can_change_and_therefore_doe_not_charge', 'complaint_allege_among_other_thing_that_airtran', 'consist_of_distribution', 'reduce_their_capacity', 'protection_rule_require_among_other_thing', 'taxiway_safety_area', 'notify_in_the_event_of_delay_of_much_than_minute', 'amended_complaint_seek_injunctive', 'facility_at_each_of_the_airport', 'change_to_environmental', 'remit_this_back_to_the_applicable_governmental_entity', 'use_to_support', 'available_seat_mile_basis_landing_fee', 'require_airline_to_i_pay_up_to_in_deny', 'increase_by_million_or_percent', 'conspire_with_delta_in_impose_bag', 'doe_not_impose', 'subject_to_a_maximum_of_per_one_way_trip', 'reservation_be_make_at_little_seven_day', 'cost_the_taxiway_safety_area_have_impose_a_annual', 'disclose_all_potential_fee_for_optional', 'capacity_or_use', 'unlike_much_of_its_competitor', 'unlike_much_of_its_competitor_southwest_doe', 'result_in_increase_land', 'department_of_transportation', 'company_be_conjurer_require', 'approximate_million_for_southwest', 'new_and_expand_component_of_the_passenger', 'airtran_and_delta_have_violate_section', 'advantage_by_differentiate', 'cost_such_a_credit', 'passenger_facility_charge', 'passenger_at_the_time_of_book_v', 'court_let_stand_the_claim', 'maximum_of_per_one_way', 'federal_aviation_administration', 'august_the_court', 'payments_be_expect', 'action_and_decision', 'business_through_the_imposition', 'access_at_slot', 'cancellation_or_diversion_of_their_flight', 'baggage_fee_must_be_disclose', 'domestic_and_foreign', 'imposition_of_a_\\ufeff1_bag', 'deduction_and_preference', 'hour_without_make_a_payment_ii_passenger', 'department_of_transportation_propose', 'primarily_a_a_result', 'fund_for_passenger', 'airfare_the_customer_will_not_be_charge_a_change', 'federal_transportation_taxis_federal', 'effort_to_reduce_the_federal_deficit', 'information_for_basic', 'grants_to_airport', 'gain_a_competitive', 'check_baggage_carriage', 'permit_and_approval', 'fare_and_the_adoption', 'extension_or_transfer_of_miles_rewards', 'additionally_when_other_airline', 'restriction_on_competitive', 'customer_service_by_show_that_southwest_understand', 'domestic_flight_begin_december', 'passenger_be_allow_to_hold_a_reservation', 'person_or_entity_in_the_united_states', 'measure_have_conjurer', 'pet_liquor_sale_advance', 'additional_federal_aviation_security', '\\ufeff1_or_2_bag', 'pay_to_airtran_and_to_delta_the_consolidated', 'expense_be_include_in_aircraft', 'fee_for_permanently_lose_luggage', 'passenger_protection_rules_conjurer', 'sherman_act_the_court_let', 'minimum_of_per_one_way', 'ancillary_service_on_their_website_and_iv', 'passenger_from_per_passenger_segment', 'baggage_allowance_and_fee_must_apply', 'asif_on_each_airline', '\\ufeff1_and_2', 'finance_may_conjurer', 'difficulty_in_obtain', 'change_and_therefore_doe', 'fee_and_other_rental', 'service_be_provide_which_be_typically_the_flight', 'passenger_trip_vi_baggage', 'damage_on_behalf_of_a_putative_class', 'advance_of_travel_iii_fare', 'increase_after_purchase_iv', 'primarily_due_to_consult', 'service_such_a_the_transportation_of_unaccompanied_minor', 'seat_assignment_call_center_service', 'check_bag_fee_for_permanently', 'charge_a_change', 'campaign_emphasize_southwest_approach', 'reimbursement_of_the_company', 'taxis_and_fee', 'require_that_i_advertise_airfares_include'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(bigram[matched_sents], common_terms=common_terms, min_count=5,threshold=5)\n",
    "trigram = Phraser(phrases)\n",
    "\n",
    "print_phrases(trigram, bigram[matched_sents], num_underscores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC NUMBER: 5\n",
      "\n",
      "ORIGINAL SENTENT: landing fee and other rental per available_seat_mile increase percent in compare to which include a airport credit of million\n",
      "\n",
      "BIGRAM: landing_fee and other rental_per_available_seat_mile increase_percent in compare to which include a airport credit of million\n",
      "\n",
      "TRIGRAM: landing_fee_and_other_rental_per_available_seat_mile increase_percent in compare to which include a airport credit of million\n"
     ]
    }
   ],
   "source": [
    "for doc_num in [5]:\n",
    "    print('DOC NUMBER: {}\\n'.format(doc_num))\n",
    "    print('ORIGINAL SENTENT: {}\\n'.format(' '.join(matched_sents[doc_num])))\n",
    "    print('BIGRAM: {}\\n'.format(' '.join(bigram[matched_sents[doc_num]])))\n",
    "    print('TRIGRAM: {}'.format(' '.join(trigram[bigram[matched_sents[doc_num]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned text to a new file for later use\n",
    "with open(AIRLINE_CLEANED_TEXT_PATH, 'w') as f:\n",
    "    for line in bigram[matched_sents]:\n",
    "        line = ' '.join(line) + '\\n'\n",
    "        line = line.encode('ascii', errors='ignore').decode('ascii')\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete below?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Phrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the_Southwest_Airlines_Rapid_Rewards_Visa_Signature_Card', 'Company’s_Rapid_Rewards', 'A-List_Preferred', 'the_Freedom_Award', 'The_Rapid_Rewards', 'a_Companion_Pass', 'A-List_and', 'the_Rapid_Rewards', 'Rapid_Rewards_Partners', 'the_Companion_Pass'}\n"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    "    ents = nlp(doc.text).ents\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    IGNORE_ENTS = ('QUANTITY','ORDINAL','CARDINAL','DATE'\n",
    "                   ,'PERCENT','MONEY','TIME')\n",
    "    ents = [ent for ent in ents if \n",
    "             (ent.label_ not in IGNORE_ENTS) and (len(ent) > 2)]\n",
    "    \n",
    "    # add underscores to combine words in entities\n",
    "    ents = [str(ent).strip().replace(' ','_') for ent in ents]\n",
    "    \n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if \n",
    "           token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    doc.extend([entity for entity in ents])\n",
    "    \n",
    "    return [str(term) for term in doc]\n",
    "\n",
    "    \n",
    "# combined terms before phrase_model (entities and/or noun chunks)\n",
    "before_phrase = []\n",
    "for sent in doc.sents:\n",
    "    text = clean_text(sent)\n",
    "    for term in text:\n",
    "        if '_' in term:\n",
    "            before_phrase.append(term)\n",
    "\n",
    "print(set(before_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the_Southwest_Airlines_Rapid_Rewards_Visa_Signature_Card', 'Company’s_Rapid_Rewards', 'A-List_Preferred', 'the_Freedom_Award', 'The_Rapid_Rewards', 'a_Companion_Pass', 'A-List_and', 'way_trip', 'the_Rapid_Rewards', 'Rapid_Rewards_Partners', 'credit_card', 'the_Companion_Pass'}\n"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    "    ents = nlp(doc.text).ents\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    IGNORE_ENTS = ('QUANTITY','ORDINAL','CARDINAL','DATE'\n",
    "                   ,'PERCENT','MONEY','TIME')\n",
    "    ents = [ent for ent in ents if \n",
    "             (ent.label_ not in IGNORE_ENTS) and (len(ent) > 2)]\n",
    "    \n",
    "    # add underscores to combine words in entities\n",
    "    ents = [str(ent).strip().replace(' ','_') for ent in ents]\n",
    "    \n",
    "    # clean text for phrase model\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc_ = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    phrase_text = [str(term) for term in doc_]\n",
    "    sent = trigram[bigram[phrase_text]]\n",
    "    phrases = []\n",
    "    for term in sent:\n",
    "        if '_' in term:\n",
    "            phrases.append(term)\n",
    "\n",
    "    # remove stops words - \n",
    "    # separate step as they are needed for the phrase model\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # add phrases and entities\n",
    "    doc.extend([entity for entity in ents])\n",
    "    clean_text = [str(term) for term in doc] + phrases\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# combined terms after phrase model\n",
    "after_phrase = []\n",
    "for sent in doc.sents:\n",
    "    text = clean_text(sent)\n",
    "    for term in text:\n",
    "        if '_' in term:\n",
    "            after_phrase.append(term)\n",
    "\n",
    "print(set(after_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline.index      airline.mm         airline.mm.index   airline_dict.dict\r\n"
     ]
    }
   ],
   "source": [
    "ls ../raw_data/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned text to a new file for later use\n",
    "with open(AIRLINE_CLEANED_TEXT_PATH, 'w') as f:\n",
    "    for line in cleaned_text:\n",
    "        line = ' '.join(line) + '\\n'\n",
    "        f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:guild]",
   "language": "python",
   "name": "conda-env-guild-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
