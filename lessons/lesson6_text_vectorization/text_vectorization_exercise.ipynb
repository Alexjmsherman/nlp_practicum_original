{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization Exercise\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "- **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "\n",
    "- **counting** the occurrences of tokens in each document.\n",
    "\n",
    "- **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "Sources: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of documents\n",
    "text = ['This is the first document'\n",
    "        , 'This is the second second document'\n",
    "        , 'And the third one'\n",
    "        , 'Is it the first document again']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - import from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an instance of countvectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# when we print vect, we see its hyperparameters\n",
    "print(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vectorizer learns the vocabulary when we fit it with our documents. \n",
    "# This means it learns the distinct tokens (terms) in the text of the documents. \n",
    "# We can observe these with the method get_feature_names\n",
    "\n",
    "vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL_SENTENCES: \n",
      " ['This is the first document', 'This is the second second document', 'And the third one', 'Is it the first document again'] \n",
      "\n",
      "FEATURE_NAMES: \n",
      " ['again', 'and', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print('ORIGINAL_SENTENCES: \\n {} \\n'.format(text))\n",
    "print('FEATURE_NAMES: \\n {}'.format(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform creates a sparse matrix, identifying the indices where terms are stores in each document\n",
    "# This sparse matrix has 4 rows and 11 columns\n",
    "\n",
    "vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t2\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://op2.github.io/PyOP2/_images/csr.svg width=1000 height=200></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix_url = 'https://op2.github.io/PyOP2/_images/csr.svg'\n",
    "iframe = '<iframe src={} width=1000 height=200></iframe>'.format(sparse_matrix_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is easier to understand when we covert the sparse matrix into a dense matrix or pandas DataFrame\n",
    "vect.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   again  and  document  first  is  it  one  second  the  third  this\n",
       "0      1    0         1      0   0   0    0       0    0      0     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# store the dense matrix\n",
    "data = vect.transform(text).toarray()\n",
    "\n",
    "# store the learned vocabulary\n",
    "columns = vect.get_feature_names()\n",
    "\n",
    "# combine the data and columns into a dataframe\n",
    "pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "A corpus of documents can thus be represented by a **matrix with one row per document and one column per token (e.g. word)** occurring in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the trained CountVectorizer to vectorize the following sentences. Create a dataframe with the dense results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = ['again we observe a document'\n",
    "               , 'the second time we have see this text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   again  and  document  first  is  it  one  second  the  third  this\n",
       "0      1    0         1      0   0   0    0       0    0      0     0\n",
       "1      0    0         0      0   0   0    0       1    1      0     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_transform\n",
    "\n",
    "- we can combine the training and transformation into a single method. This is a common process in the sklearn api, as we often want to learn something from a training data set and apply the results to testing or production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://karlrosaen.com/ml/learning-log/2016-06-20/pipeline-diagram.png width=800 height=550></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_url = 'http://karlrosaen.com/ml/learning-log/2016-06-20/pipeline-diagram.png'\n",
    "iframe = '<iframe src={} width=800 height=550></iframe>'.format(pipeline_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize the Transformer\n",
    "\n",
    "During the process of vectorizing the text, we can apply numerous transformations to modify the text and resulting vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lowercase\n",
    "- boolean, True by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Is', 'This', 'again', 'document', 'first', 'is', 'it', 'one', 'second', 'the', 'third']\n"
     ]
    }
   ],
   "source": [
    "# by instantiating CountVectorizer with differnt parameters, we can change the vocabulary\n",
    "# lowercase determines if all words should be lowercase, setting it to False includes uppercase words\n",
    "\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop_words\n",
    "\n",
    "- string {‘english’}, list, or None (default)\n",
    " - If None, no stop words will be used. \n",
    " - If ‘english’, a built-in stop word list for English is used.\n",
    " - If list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'second']\n"
     ]
    }
   ],
   "source": [
    "# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'document', 'is', 'it', 'one', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\n",
    "vect = CountVectorizer(stop_words=['first','second','third'])\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary\n",
    "\n",
    "- Mapping or iterable, optional\n",
    "- Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'second', 'third']\n"
     ]
    }
   ],
   "source": [
    "# stops words determine if we should include common words (e.g. and, is, the) which show up in most documents\n",
    "vect = CountVectorizer(vocabulary=['first','second','third'])\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_features\n",
    "- int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top  max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'second', 'the']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=5)\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_df\n",
    "- float in range [0.0, 1.0] or int, default=1.0\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again', 'and', 'first', 'it', 'one', 'second', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_df=.5)\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_df\n",
    "\n",
    "- float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=.5)\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram_range\n",
    "\n",
    "- tuple (min_n, max_n)\n",
    "\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and the', 'first document', 'is the', 'the first', 'this is']\n"
     ]
    }
   ],
   "source": [
    "# max features determines the maximum number of features to display\n",
    "vect = CountVectorizer(ngram_range=(2,2), max_features=5)\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary\n",
    "\n",
    "- boolean, default=False\n",
    "- If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max features determines the maximum number of features to display\n",
    "vect = CountVectorizer(binary=True)\n",
    "vect.fit_transform(['Two Two words words']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyzer\n",
    "\n",
    "- String, {‘word’, ‘char’, ‘char_wb’} or callable\n",
    "- Specifies whether to use n_grams of words or characters\n",
    "- Character n_grams are useful in certain content, such as genomics with DNA sequences (e.g. GCTATCAFF...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a', ' d', ' f', ' i', ' o', ' s', ' t', 'ag', 'ai', 'an', 'co', 'cu', 'd ', 'do', 'e ', 'ec', 'en', 'fi', 'ga', 'he', 'hi', 'in', 'ir', 'is', 'it', 'me', 'nd', 'ne', 'nt', 'oc', 'on', 'rd', 'rs', 's ', 'se', 'st', 't ', 'th', 'um']\n"
     ]
    }
   ],
   "source": [
    "# max features determines the maximum number of features to display\n",
    "vect = CountVectorizer(analyzer='char', ngram_range=(2,2))\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Bag of Words representation\n",
    "\n",
    "A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn’t account for potential misspellings or word derivations.\n",
    "\n",
    "N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n",
    "\n",
    "One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations.\n",
    "\n",
    "For example, let’s say we’re dealing with a corpus of two documents: ['words', 'wprds']. The second document contains a misspelling of the word ‘words’. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn attributes are often provided to store information of the instance of the transformer or model. \n",
    "\n",
    "Many attributes are only available after the model is fit. For instance the learned vocabulary does not exist in Countvectorizer until text data has been provided with the fit method. Until the data is provided these attributes do not exist. The notation for these learned attributes is a trailing underscore after the attribute name (e.g. vocabulary_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'is', 'second', 'the']\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=5)\n",
    "vect.fit(text)\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vocabulary_\n",
    "\n",
    "- dict\n",
    "- A mapping of terms to feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'first': 1, 'is': 2, 'second': 3, 'the': 4}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop\\__words_\\_ \n",
    "- set\n",
    "- Terms that were ignored because they either:\n",
    " - occurred in too many documents (max_df)\n",
    " - occurred in too few documents (min_df)\n",
    " - were cut off by feature selection (max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'again', 'and', 'it', 'one', 'third', 'this'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Frequency Problems\n",
    "\n",
    "\"The **main problem with the term-frequency approach is that it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms.**\n",
    "The basic intuition is that a term that occurs frequently in many documents is not a good discriminator; the important question here is: why would you, in a classification problem for instance, emphasize a term which is almost present in the entire corpus of your documents ?\n",
    "\n",
    "The tf-idf weight comes to solve this problem. **What tf-idf gives is how important is a word to a document**\n",
    "in a collection, and that’s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection. **What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another isn’t 10 times more important than it, that’s why tf-idf uses the logarithmic scale to do that.\"**\n",
    "\n",
    "Source: http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: \n",
    "\n",
    "- tf-idf(t,d) = tf(t,d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first document',\n",
       " 'This is the second second document',\n",
       " 'And the third one',\n",
       " 'Is it the first document again']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.418127</td>\n",
       "      <td>0.516470</td>\n",
       "      <td>0.418127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.837067</td>\n",
       "      <td>0.218408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288477</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.519263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331439</td>\n",
       "      <td>0.409393</td>\n",
       "      <td>0.331439</td>\n",
       "      <td>0.519263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      again       and  document     first        is        it       one  \\\n",
       "0  0.000000  0.000000  0.418127  0.516470  0.418127  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.267144  0.000000  0.267144  0.000000  0.000000   \n",
       "2  0.000000  0.552805  0.000000  0.000000  0.000000  0.000000  0.552805   \n",
       "3  0.519263  0.000000  0.331439  0.409393  0.331439  0.519263  0.000000   \n",
       "\n",
       "     second       the     third      this  \n",
       "0  0.000000  0.341846  0.000000  0.516470  \n",
       "1  0.837067  0.218408  0.000000  0.329977  \n",
       "2  0.000000  0.288477  0.552805  0.000000  \n",
       "3  0.000000  0.270973  0.000000  0.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "pd.DataFrame(tfidf_vect.fit_transform(text).toarray(), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Analysis\n",
    "As we look at the tfidf score (which have a range of 0-1), high score occur for words that show up frequently in specific sentence but infrequenty overall. Low score occur in words that show up frequenty across all documents.\n",
    "\n",
    "- **'Second' has a high score** as it shows up twice in document two and not in any other documents\n",
    "- **'The' has a low score** as it show up in all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### term frequency (tf)\n",
    "\n",
    "How often does each term exist in each document. \n",
    "\n",
    "Term frequency is the numerator; thus, the tfidf score for a term increases in documents where it is frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 0 0 1 0 1]\n",
      " [0 0 1 0 1 0 0 2 1 0 1]\n",
      " [0 1 0 0 0 0 1 0 1 1 0]\n",
      " [1 0 1 1 1 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tf = vect.fit_transform(text).toarray()\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inverse document frequency (idf)\n",
    "\n",
    "Calculation: log(\\# document in the corpus / # documents where the term appears)\n",
    "\n",
    "- **The # of documents in the corpus has no effect** as it is the same for all terms\n",
    "- **As the # of documents in which the term appears increases, the idf decreases**; thus terms that show up in many different documents (e.g. stop words) recieve low tfidf scores as they are not important terms to define the meaning of the document \n",
    "- As a sub-linear function, we take the **log because the relevance does not increase proportionally with the term frequency**. As an example if a term shows up in 1M docs or in 2M docs, the effect is not the same as if it has shown up in 1 doc or 2 docs times respectively. In other words there is a relative threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://www.science4all.org/wp-content/uploads/2013/10/Graph-of-Logarithm-and-Exponential1.png width=500 height=350></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logarithm_url = 'http://www.science4all.org/wp-content/uploads/2013/10/Graph-of-Logarithm-and-Exponential1.png'\n",
    "iframe = '<iframe src={} width=500 height=350></iframe>'.format(logarithm_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.38629436  1.38629436  0.28768207  0.69314718  0.28768207  1.38629436\n",
      "  1.38629436  0.69314718  0.          1.38629436  0.69314718]\n"
     ]
    }
   ],
   "source": [
    "print( np.log(len(tf) / tf.sum(axis=0)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 2, 3, 1, 1, 2, 4, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we use sum(axis=0) we take the sum of each column\n",
    "# as opposed to a scalar sum (single # result) of all values\n",
    "tf.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn calculation modifications\n",
    "\n",
    "scikit-learn further modifies the caluclation for adding one to the numerator, denomication, and log to avoid divide by zero errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.91629073  1.91629073  1.22314355  1.51082562  1.22314355  1.91629073\n",
      "  1.91629073  1.51082562  1.          1.91629073  1.51082562]\n"
     ]
    }
   ],
   "source": [
    "idf = np.log( (len(tf)+1) / (tf.sum(axis=0)+1) ) + 1\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.91629073  1.91629073  1.22314355  1.51082562  1.22314355  1.91629073\n",
      "  1.91629073  1.91629073  1.          1.91629073  1.51082562]\n"
     ]
    }
   ],
   "source": [
    "# value as stored from sklearn in tfidf_vect\n",
    "print(tfidf_vect.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### term frequency * inverse document frequency (tf*idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.021651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.000000  1.223144  1.510826  1.223144  0.000000  0.000000   \n",
       "1  0.000000  0.000000  1.223144  0.000000  1.223144  0.000000  0.000000   \n",
       "2  0.000000  1.916291  0.000000  0.000000  0.000000  0.000000  1.916291   \n",
       "3  1.916291  0.000000  1.223144  1.510826  1.223144  1.916291  0.000000   \n",
       "\n",
       "         7    8         9         10  \n",
       "0  0.000000  1.0  0.000000  1.510826  \n",
       "1  3.021651  1.0  0.000000  1.510826  \n",
       "2  0.000000  1.0  1.916291  0.000000  \n",
       "3  0.000000  1.0  0.000000  0.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = pd.DataFrame(tf*idf)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### term vector normalization\n",
    "\n",
    "The use of the simple tfidf does not account for the length of the document. Additionally it provides opportunities for spammers to repeat the term many times to make it seem more important.\n",
    "\n",
    "To solve these issues, we normalize each vector. By default TfidfVectorizer uses an 'l2' normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.832581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.510826</td>\n",
       "      <td>1.223144</td>\n",
       "      <td>1.916291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.000000  1.223144  1.510826  1.223144  0.000000  0.000000   \n",
       "1  0.000000  0.000000  1.223144  0.000000  1.223144  0.000000  0.000000   \n",
       "2  0.000000  1.916291  0.000000  0.000000  0.000000  0.000000  1.916291   \n",
       "3  1.916291  0.000000  1.223144  1.510826  1.223144  1.916291  0.000000   \n",
       "\n",
       "         7    8         9         10  \n",
       "0  0.000000  1.0  0.000000  1.510826  \n",
       "1  3.832581  1.0  0.000000  1.510826  \n",
       "2  0.000000  1.0  1.916291  0.000000  \n",
       "3  0.000000  1.0  0.000000  0.000000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf*idf is equivalent to using TfidfVectorizer without a norm\n",
    "tfidf_vect = TfidfVectorizer(norm=None)\n",
    "pd.DataFrame(tfidf_vect.fit_transform(text).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.418127</td>\n",
       "      <td>0.516470</td>\n",
       "      <td>0.418127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.341846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.51647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.76986</td>\n",
       "      <td>0.254781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.38493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.288477</td>\n",
       "      <td>0.552805</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.519263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331439</td>\n",
       "      <td>0.409393</td>\n",
       "      <td>0.331439</td>\n",
       "      <td>0.519263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.270973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.000000  0.418127  0.516470  0.418127  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.311634  0.000000  0.311634  0.000000  0.000000   \n",
       "2  0.000000  0.552805  0.000000  0.000000  0.000000  0.000000  0.552805   \n",
       "3  0.519263  0.000000  0.331439  0.409393  0.331439  0.519263  0.000000   \n",
       "\n",
       "        7         8         9        10  \n",
       "0  0.00000  0.341846  0.000000  0.51647  \n",
       "1  0.76986  0.254781  0.000000  0.38493  \n",
       "2  0.00000  0.288477  0.552805  0.00000  \n",
       "3  0.00000  0.270973  0.000000  0.00000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "pd.DataFrame(normalize(tfidf, norm='l2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting width=1100 height=300></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlalchemy_url = 'http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting'\n",
    "iframe = '<iframe src={} width=1100 height=300></iframe>'.format(sqlalchemy_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25\n",
    "\n",
    "BM25 is often a better algorithm that tfidf to determine term importance as it takes that document length into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=http://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/ width=1100 height=300></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://opensourceconnections.com/blog/2015/10/16/bm25-the-next-generation-of-lucene-relevation/'\n",
    "iframe = '<iframe src={} width=1100 height=300></iframe>'.format(url)\n",
    "HTML(iframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
