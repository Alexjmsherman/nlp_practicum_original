{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation Lesson\n",
    "\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Objectives:\n",
    "- Discuss python String formatting\n",
    "- Discuss Client-side versus Server-side programming\n",
    "- Learn how to request data (html, css, javascript) from a web page\n",
    "- Structure the html with BeautifulSoup\n",
    "- Identify, extract, and store selected elements from the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../../raw_data/images/lesson2_automation.png\", width=800, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package to view websites inside the Jupyter Notebook\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use magic command to print working directory\n",
    "# confirm you are in lesson2_automation directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%less ../../config.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "\n",
    "SELENIUM_CHROMEDRIVER_PATH = config['AUTOMATION']['SELENIUM_CHROMEDRIVER_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python String Formatting\n",
    "\n",
    "We will use formatting to add placeholders into strings in many of the lessons. For this, we will use str.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_formatting_url = 'https://pyformat.info/'\n",
    "iframe = '<iframe src={} width=1100 height=300></iframe>'.format(string_formatting_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'This is a python string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'We can add a parameter {}'.format('into the string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'format {} the braces'.format('fills in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'.format() allows multiple parameters, of any datatype, param: {2}, param: {2},  param: {2}'.format('one', 2, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'add {:10} with a {:10} then number'.format('spaces','colon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting is useful for populating strings as part of a loop\n",
    "urls = ['url1','url2','url3']\n",
    "\n",
    "for url in urls:\n",
    "    print('Successfully requested data from {}'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REQUESTS\n",
    "\n",
    "Requests is a Python HTTP library, released under the Apache2 License. The goal of the project is to make HTTP requests simpler and more human-friendly. We will use requests to get html, css, and javascript from webpages to collect data from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_url = 'http://docs.python-requests.org/en/master/'\n",
    "iframe = '<iframe src={} width=1100 height=300></iframe>'.format(requests_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# set a url for opm.gov\n",
    "opm_url = r'https://www.opm.gov/'\n",
    "\n",
    "# r is the common name for a requests instance\n",
    "r = requests.get(opm_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two commonly used methods for a request-response between a client and server are: GET and POST.\n",
    "- GET - Requests data from a specified resource\n",
    "- POST - Submits data to be processed to a specified resource\n",
    "\n",
    "\n",
    "#### Client-side vs Server-side Programming Languages\n",
    "\n",
    "Web development is all about communication and data exchange. This communication takes place via two parties over the HTTP protocol.\n",
    "\n",
    "- Server: The Server is responsible for serving the web pages depending on the client/end user requirement. It can be either static or dynamic.\n",
    "- Client: A client is a party that requests pages from the server and displays them to the end user. In general a client program is a web browser.\n",
    "\n",
    "source: http://www.c-sharpcorner.com/UploadFile/2072a9/client-side-vs-server-side-programming-languages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client_server_url = 'https://www.afterhoursprogramming.com/tutorial/javascript/javascript-overview/'\n",
    "iframe = '<iframe src={} width=700 height=400></iframe>'.format(client_server_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the response object from requests which contains a server's response to an HTTP request\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what methods and attributes exist for the requests object\n",
    "print(dir(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if r.status_code == 200:\n",
    "    print('success')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status Code Explanation    \n",
    "When a browser requests a service from a web server, an error might occur. The first digit of the status code specifies one of five standard classes of responses.\n",
    "\n",
    "- 1xx: Information\n",
    "- 2xx: Successful\n",
    "- 3xx: Redirection\n",
    "- 4xx: Client Error\n",
    "- 5xx: Server Error\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests text and encoding\n",
    "\n",
    "Requests will automatically decode content from the server. Most unicode charsets are seamlessly decoded. When you make a request, Requests makes educated guesses about the encoding of the response based on the HTTP headers. \n",
    "\n",
    "The text encoding guessed by Requests is used when you access r.text. You can find out what encoding Requests is using, and change it, using the r.encoding property:\n",
    "\n",
    "- What is the difference between unicode and ascii: https://stackoverflow.com/questions/19212306/whats-the-difference-between-ascii-and-unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup\n",
    "\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_url = 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'\n",
    "iframe = '<iframe src=' + bs_url + ' width=1100 height=300></iframe>'\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# structure the text of the request object\n",
    "b = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup structures the text, removing unnecessary \n",
    "# newline \\n and tab \\t characters\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example HTML\n",
    "\n",
    "Before we explore the full HMTL, let's work with some example HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_html = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang='en'>\n",
    "\n",
    "    <head>\n",
    "        <title>The ML Guild</title>\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <h1 id='headline'>Machine Learning Guild Overview</h1>\n",
    "            <p class='description'>The ML guild provides rich learning \n",
    "            content around machine learning, including self-guided courses, \n",
    "            curriculum-based learning programs, and mentorship by more \n",
    "            advanced practitioners</p>\n",
    "        <h2>Courses</h2>\n",
    "\n",
    "        <p>Below are the ML Guild Tracks:</p>\n",
    "\n",
    "        <ul>\n",
    "            <li class='first'>Explorer</li>\n",
    "            <li class='second'>Apprentice</li>\n",
    "            <li class='third'>Master</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "\n",
    "    </html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common BeautifulSoup Syntax\n",
    "\n",
    "#### Methods\n",
    "- find: find only the first specified html tag that meets a specified condition\n",
    "- find_all: find all html tags that meets a specified condition\n",
    "\n",
    "#### Parameters\n",
    "- attrs: dict of attributes (e.g. id, class) to filter relevant html\n",
    "\n",
    "#### Attributes\n",
    "- text: only view the text in the html (ignore the html itself) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Beautiful Soup supports the HTML parser included in Python’s standard library, \n",
    "# but it also supports a number of third-party Python parsers. \n",
    "# One is the lxml parser, which is fast and lenient (i.e. it will not crash it the html is not formatted correctly)\n",
    "\n",
    "b = BeautifulSoup(example_html, 'lxml')\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the h2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text of the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text of the h2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text of a list item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the li elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first li element from a find_all li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively print each each from the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the li with a class of 'first'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the li with a class of 'second'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the term 'headline' from the h1 tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text (only) for list element with class='third'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the text (only) of all the paragraphs in a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the paragraph with the class description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robots.txt\n",
    "\n",
    "\"WWW Robots (also called wanderers or spiders) are programs that traverse many pages in the World Wide Web by recursively retrieving linked pages. For more information see the robots page.\n",
    "In 1993 and 1994 there have been occasions where robots have visited WWW servers where they weren't welcome for various reasons. Sometimes these reasons were robot specific, e.g. certain robots swamped servers with rapid-fire requests, or retrieved the same files repeatedly. In other situations robots traversed parts of WWW servers that weren't suitable, e.g. very deep virtual trees, duplicated information, temporary information, or cgi-scripts with side-effects (such as voting).\"\n",
    "\n",
    "SOURCE: http://www.robotstxt.org/orig.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robots_url = 'http://www.robotstxt.org/orig.html'\n",
    "iframe = '<iframe src={} width=1100 height=400></iframe>'.format(robots_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import robotparser\n",
    " \n",
    "rp = robotparser.RobotFileParser()  # instantiate robotparser\n",
    "rp.set_url(\"https://www.facebook.com/robots.txt\")  # set the path to the robots.txt file\n",
    "rp.read()  # read the robots.txt file\n",
    "rp.can_fetch('*', \"https://www.facebook.com\")  # check if you are allowed to fetch a specific page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url(\"https://www2.deloitte.com/robots.txt\")\n",
    "rp.read()\n",
    "rp.can_fetch('*', \"https://www2.deloitte.com/us/en.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url(\"http://www.annualreports.com/robots.txt\")\n",
    "rp.read()\n",
    "rp.can_fetch(\"*\", \"http://www.annualreports.com/Company/southwest-airlines-co\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url('https://www.opm.gov/robots.txt')\n",
    "rp.read()\n",
    "rp.can_fetch(\"*\", \"https://www.opm.gov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Extract blog posts from OPM.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REVIEW: GET OPM HTML\n",
    "\n",
    "# set a url for the OPM website homepage\n",
    "url = r'https://www.opm.gov/'\n",
    "\n",
    "# r is the common name for a requests instance\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the html from deloitte\n",
    "b = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first blog post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the blog Title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the date the blog was posted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the blog text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the blog posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data\n",
    "\n",
    "We will use a common python data stucture (e.g. list or dict) to store the data for later use. \n",
    "\n",
    "Below is a common data collection pattern:\n",
    "- A. set an empty data structure to store data\n",
    "- B. iterate through an existing data set container\n",
    "- C. extract selected pieces of information, disregard the rest of the data\n",
    "- D. add the selected information to the created data structure\n",
    "- E. view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = []  # A \n",
    "\n",
    "for blog in blogs: # B\n",
    "    headline = blog.find('div', attrs={'class':'Blog_Title'}).text   # C\n",
    "    all_headlines.append(headline)  # D\n",
    "\n",
    "all_headlines  # E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension - a pythonic idiom to accomplish the same task in one line\n",
    "# resource: http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#list-comprehensions\n",
    "\n",
    "[blog.find('div', attrs={'class':'Blog_Title'}).text for blog in blogs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect ZIP URLS from OPM.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://www.opm.gov/data/Index.aspx?tag=FedScope'\n",
    "r = requests.get(url)\n",
    "b = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the html for the table of files (HINT: look for the class DataTable)\n",
    "data_table = b.find('table', attrs={'class':'DataTable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the html for the first row of files (HINT: make sure to skip the table headers)\n",
    "row = data_table.find_all('tr')[1]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 'td' element with the file name\n",
    "filename = row.find('td').text\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 'td' element with the .zip link (HINT: look for 'a href')\n",
    "url = data_table.find_all('td')[2]\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the zip url\n",
    "url_end = url.find('a')['href']\n",
    "url_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the base url\n",
    "BASE_URL = r'https://www.opm.gov'\n",
    "url = ''.join([BASE_URL, url_end])\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exception Handling\n",
    "0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try - Except to handle expected errors\n",
    "try:\n",
    "   0/0\n",
    "except ZeroDivisionError:\n",
    "    print('successfully caught error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Combine the above code to collect all the zip urls into differnet a list (exercise 1) then into a dict (exercise 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify, extract, and store all zip urls\n",
    "# store them in a list called zip_urls\n",
    "\n",
    "BASE_URL = r'https://www.opm.gov'\n",
    "zip_urls = []\n",
    "data_table = b.find('table', attrs={'class':'DataTable'})\n",
    "\n",
    "\n",
    "for row in data_table.find_all('tr')[1:]:\n",
    "    # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify, extract, and store all filenames and zip urls\n",
    "# store them in a dict called zip_urls\n",
    "\n",
    "BASE_URL = r'https://www.opm.gov'\n",
    "zip_urls = {}\n",
    "data_table = b.find('table', attrs={'class':'DataTable'})\n",
    "\n",
    "for row in data_table.find_all('tr')[1:]: # skip table headers\n",
    "    # TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Material\n",
    "\n",
    "### Selenium - Web Browser Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Download ChromeDriver:\n",
    "- https://chromedriver.storage.googleapis.com/index.html?path=2.37/\n",
    "\n",
    "In Git Bash Type the following:\n",
    "- conda install -c conda-forge selenium\n",
    "- conda install -c conda-forge googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm path to selenium executable\n",
    "SELENIUM_CHROMEDRIVER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = robotparser.RobotFileParser()\n",
    "rp.set_url('https://www.opm.gov/data/Index.aspx?tag=FedScope')\n",
    "rp.read()\n",
    "rp.can_fetch(\"*\", \"https://www.opm.gov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which browser to use (Chrome)\n",
    "# https://chromedriver.storage.googleapis.com/index.html?path=2.25/\n",
    "browser = webdriver.Chrome(SELENIUM_CHROMEDRIVER_PATH)\n",
    "browser.implicitly_wait(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to OPM.gov\n",
    "browser.get(r'https://www.opm.gov/data/Index.aspx?tag=FedScope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an item in the sort by box\n",
    "browser.find_element_by_xpath(\n",
    "    r'//*[@id=\"ctl01_ctl00_MainContentPlaceHolder_MainContentPlaceHolder_SortCol\"]/option[2]').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Press 'go' button\n",
    "browser.find_element_by_xpath(\n",
    "    r'//*[@id=\"ctl01_ctl00_MainContentPlaceHolder_MainContentPlaceHolder_Filter\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type and submit a query in the search box\n",
    "inputbox = browser.find_element_by_xpath(r'//*[@id=\"ctl01_ctl00_MainContentPlaceHolder_MainContentPlaceHolder_SearchTerm\"]')\n",
    "      \n",
    "inputbox.send_keys('OPM')\n",
    "inputbox.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return to OPM.gov\n",
    "browser.get(r'https://www.opm.gov/data/Index.aspx?tag=FedScope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the assessment drop down\n",
    "browser.find_element_by_xpath(\n",
    "        r'//*[@id=\"SecondaryNavigation\"]/li[1]/a[2]'\n",
    "        ).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_first_zip_link\n",
    "html = browser.find_element_by_xpath(\n",
    "    r'//*[@id=\"ctl01_ctl00_MainContentDiv\"]/table/tbody/tr[2]/td[3]/span/a')\n",
    "link = html.get_attribute('href')\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the browser\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML\n",
    "xml_url = r'https://raw.githubusercontent.com/Alexjmsherman/python_for_data_analysis/master/Data_Scraping_APIs_and_Automation/data/example_xml.xml'\n",
    "r = requests.get(xml_url)\n",
    "b = BeautifulSoup(r.text, 'xml')\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the fourth topic\n",
    "b.find_all('NAME')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data from google maps api\n",
    "# documentation: https://developers.google.com/maps/documentation/geocoding/start\n",
    "address = 'Cosi Rosslyn'\n",
    "api_url = r'https://maps.googleapis.com/maps/api/geocode/xml?address={}'.format(address)\n",
    "r = requests.get(api_url)\n",
    "b = BeautifulSoup(r.text, 'xml')\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the status code\n",
    "b.find('status').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the postal code\n",
    "address_components = b.find_all('address_component')\n",
    "for address in address_components:\n",
    "    address_type = address.find('type').text\n",
    "    if 'postal_code' in address_type:\n",
    "        print(address.find('long_name').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the latitude and longitude\n",
    "latitude = b.find('location').find('lat').text\n",
    "longitude = b.find('location').find('lng').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "import json\n",
    "\n",
    "# The missing JSON inspector for chrome \n",
    "# https://chrome.google.com/webstore/detail/the-missing-json-inspecto/hhffklcokfpbcajebmnpijpkaeadlgfn\n",
    "address = 'Cosi Rosslyn'\n",
    "api_json_url = r'https://maps.googleapis.com/maps/api/geocode/json?address={}'.format(address)\n",
    "r = requests.get(api_json_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latitude and longitude from json\n",
    "j = json.loads(r.text)\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude\n",
    "print(j['results'][0]['geometry']['location']['lat'])\n",
    "\n",
    "# longitude\n",
    "print(j['results'][0]['geometry']['location']['lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a python api wrapper package for google maps\n",
    "# https://github.com/googlemaps/google-maps-services-python\n",
    "import googlemaps  \n",
    "\n",
    "gmaps = googlemaps.Client(key='INSERT YOUR API KEY HERE')\n",
    "gmaps.reverse_geocode((latitude,longitude))\n",
    "gmaps.places('Cosi', location=(latitude, longitude), radius=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "\n",
    "\"Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\"\n",
    "\n",
    "##### How does Scrapy compare to BeautifulSoup or lxml?\n",
    "\n",
    "\"BeautifulSoup and lxml are libraries for parsing HTML and XML. Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\n",
    "\n",
    "Scrapy provides a built-in mechanism for extracting data (called selectors) but you can easily use BeautifulSoup (or lxml) instead, if you feel more comfortable working with them. After all, they’re just parsing libraries which can be imported and used from any Python code.\n",
    "\n",
    "In other words, comparing BeautifulSoup (or lxml) to Scrapy is like comparing jinja2 to Django.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy_url = 'https://doc.scrapy.org/en/latest/'\n",
    "iframe = '<iframe src=' + scrapy_url + ' width=1100 height=300></iframe>'\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AnnualReportsSpider(scrapy.Spider):\n",
    "    name = \"annual_reports\"\n",
    "    # create a list of all the pages with alphabetical company names\n",
    "    start_urls = ['http://www.annualreports.com/Companies?a={}'.format(letter) for letter in ['A','B']]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # get the url of the company specific webpage\n",
    "        for company in response.css('#wrapper > section > table > tbody > tr > td > a::attr(href)').extract():\n",
    "            if company is not None:\n",
    "                # go to each company page to extract the names of all the annual reports\n",
    "                yield response.follow(company, self.parse_reports)\n",
    "\n",
    "    def parse_reports(self, response):\n",
    "        # extract the company name \n",
    "        company_name = str(response.css('#wrapper > div.company-header.portal-header > h1::text')[0].extract())\n",
    "        # collect all available annual reports\n",
    "        reports = [report for report in response.css('#wrapper > div.venders-block > article > div.content-holder.content-archive > ul > li > div.text-holder > ul > li:nth-child(1)::text').extract()]\n",
    "        yield {company_name:str(reports)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following from Git Bash:\n",
    "- **scrapy runspider annual_reports_spider.py -o annual_reports.json**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guild",
   "language": "python",
   "name": "guild"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
